# -*- coding: utf-8 -*-
"""TASK#1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u-kmG1USY74r39hqZBePjvvD3GZOJkcJ

# **LetsGrowMore**

---



---
# ***Data Science Internship***

---



---

## `Author: UMER FAROOQ`
## `Task Level: Beginner Level`
## `Task Number: 1`
## `Task Title: Iris Flower Classification`


## `Language: Python`
## `IDE: Google Colab`

# **Steps**:

# **Step:1**
***Importing Libraries***
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import seaborn as sns
sns.set_palette('husl')
import matplotlib.pyplot as plt
# %matplotlib inline

from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC

"""# **Step:2**
***Loading the Dataset:*** I have pick the dataset from the following link. You can download the dataset as well as you can use by URL.
"""

url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv'

# Creating the list of column name:

column_name = ['sepal-lenght','sepal-width','petal-lenght','petal-width','class']

# Pandas read_csv() is used for reading the csv file:

dataset = pd.read_csv(url, names = column_name)

"""# **Step:3**
***Dataset Summarizing:*** Check the structure/shape of data on which we have to work on.
"""

dataset.shape

"""This shows that we have: 

1.   150 rows,
2.   5 columns.

Thats enough for our Beginner Project.
*Displaying the First 5 records:*



"""

dataset.head()

"""Pandas info() method prints information about a DataFrame such as datatypes, cols, NAN values and usage of memory:"""

dataset.info()

dataset.isnull()

# Returns no. of missing records/values
dataset.isnull().sum()

""" Pandas describe() is used to view some basic statistical details like percentile, mean, std etc. of a data frame or a series of numeric values: """

dataset.describe()

"""**Now let’s check the number of rows that belongs to each class:**"""

dataset['class'].value_counts() # No of records/samples in each class

"""The above outputs shows that each class of flowers has 50 rows.

# **Step: 4 Data Visualization**

---



---

Data visualization is the process of translating large data sets and metrics into charts, graphs and other visuals.

---



---

**Violin Plot:** Plotting the violin plot to check the comparison of a variable distribution:
"""

sns.violinplot(y='class', x='sepal-lenght', data=dataset, inner='quartile')
plt.show()
print('\n')

sns.violinplot(y='class', x='sepal-width', data=dataset, inner='quartile')
plt.show()
print('\n')

sns.violinplot(y='class', x='petal-lenght', data=dataset, inner='quartile')
plt.show()
print('\n')

sns.violinplot(y='class', x='petal-width', data=dataset, inner='quartile')
plt.show()
print('\n')

"""Above-plotted violin plot says that Iris-Setosa class is having a smaller petal length and petal width as compared to other class.

**Pair Plot:** Plotting multiple pairwise bivariate distributions in a dataset using pairplot:
"""

sns.pairplot(dataset, hue='class', markers='+')
plt.show()

"""From the above, we can see that Iris-Setosa is separated from both other species in all the features.

**Heatmap:** Plotting the heatmap to check the correlation.
**dataset.corr()** is used to find the pairwise correlation of all columns in the dataframe.
"""

plt.figure(figsize=(8,5))
sns.heatmap(dataset.corr(), annot=True, cmap= 'PuOr')
plt.show()

"""# **Step: 5 Model Construction (Splitting, Training and Model Creation)**

---

---


**SPLITTING THE DATASET:**
X have dependent variables.
Y have an independent variables.

"""

x= dataset.drop(['class'], axis=1)
y= dataset['class'] # Class is an independent variable

print('X shape: {}\nY Shape: {}'.format(x.shape, y.shape))

"""The output shows that X has 150 records/rows and 4 cols, whereas, Y has 150 records and only 1 col.

**TRAINING THE TEST SPLIT:**
Splitting our dataset into train and test using train_test_split(), what we are doing here is taking 80% of data to train our model, and 20% that we will hold back as a validation dataset:
"""

x_train, x_test, y_train, y_test = train_test_split (x, y, test_size=0.20, random_state=1)

"""**MODEL CONSTRUCTION PART:1:**
We have no idea which algorithms might work best in this situation.
Let's run each algorithm in a loop and print its accuracy so we can choose the best one. Following are the algorithms:

1.   Logistic Regression (LR)
2.   Linear Discriminant Analysis (LDA)

1.   K-Nearest Neighbors (KNN).
2.   Classification and Regression Trees (CART).

1.   Gaussian Naive Bayes (NB).
2.   Support Vector Machines (SVM).






"""

models = []
models.append(('LR', LogisticRegression()))
models.append(('LDA', LinearDiscriminantAnalysis()))
models.append(('KNN', KNeighborsClassifier()))
models.append(('CART', DecisionTreeClassifier()))
models.append(('NB', GaussianNB()))
models.append(('SVC', SVC(gamma='auto')))
# evaluate each model in turn
results = []
model_names = []
for name, model in models:
    kfold = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)
    cv_results = cross_val_score(model, x_train, y_train, cv=kfold, scoring='accuracy')
    results.append(cv_results)
    model_names.append(name)
    print('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))

"""***Support Vector Classifier (SVC) is performing better than other algorithms.
Let’s train SVC model on our training set and predict on test set in the next step.***

**MODEL CONSTRUCTION PART:2**

We are defining our SVC model and passing gamma as auto.

After that fitting/training the model on X_train and Y_train using .fit() method.

Then we are predicting on X_test using .predict() method.
"""

model = SVC(gamma='auto')
model.fit(x_train, y_train)
prediction = model.predict(x_test)

"""**Now checking the accuracy of the model using accuracy_score(y_test, prediction).**

y_test is actually values of x_test
prediction: it predicts values of x_test as mentioned earlier (*then we are predicting on X_test using .predict() method*).

**Printing out the classfication report using:** classification_report(y_test, prediction)


"""

print(f"Test Accuracy: {accuracy_score(y_test, prediction)} \n")
print(f'Classification Report:\n \n {classification_report(y_test, prediction)}')